{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b58db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import os.path\n",
    "import torchtext\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from torch import optim\n",
    "from itertools import cycle\n",
    "from torchtext import vocab\n",
    "from konlpy.tag import Mecab\n",
    "from torchinfo import summary\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchaudio.models import Conformer\n",
    "from timeit import default_timer as timer\n",
    "from soyspacing.countbase import CountSpace\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchmetrics.functional import word_error_rate\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73564d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "__all__ = [\"Conformer\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class _ConvolutionModule(torch.nn.Module):\n",
    "    r\"\"\"Conformer convolution module.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input dimension.\n",
    "        num_channels (int): number of depthwise convolution layer input channels.\n",
    "        depthwise_kernel_size (int): kernel size of depthwise convolution layer.\n",
    "        dropout (float, optional): dropout probability. (Default: 0.0)\n",
    "        bias (bool, optional): indicates whether to add bias term to each convolution layer. (Default: ``False``)\n",
    "        use_group_norm (bool, optional): use GroupNorm rather than BatchNorm. (Default: ``False``)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_channels: int,\n",
    "        depthwise_kernel_size: int,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        use_group_norm: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if (depthwise_kernel_size - 1) % 2 != 0:\n",
    "            raise ValueError(\"depthwise_kernel_size must be odd to achieve 'SAME' padding.\")\n",
    "        self.layer_norm = torch.nn.LayerNorm(input_dim)\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(\n",
    "                input_dim,\n",
    "                2 * num_channels,\n",
    "                1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            torch.nn.GLU(dim=1),\n",
    "            torch.nn.Conv1d(\n",
    "                num_channels,\n",
    "                num_channels,\n",
    "                depthwise_kernel_size,\n",
    "                stride=1,\n",
    "                padding=(depthwise_kernel_size - 1) // 2,\n",
    "                groups=num_channels,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            torch.nn.GroupNorm(num_groups=1, num_channels=num_channels)\n",
    "            if use_group_norm\n",
    "            else torch.nn.BatchNorm1d(num_channels),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv1d(\n",
    "                num_channels,\n",
    "                input_dim,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=bias,\n",
    "            ),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): with shape `(B, T, D)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output, with shape `(B, T, D)`.\n",
    "        \"\"\"\n",
    "        x = self.layer_norm(input)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.sequential(x)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class _FeedForwardModule(torch.nn.Module):\n",
    "    r\"\"\"Positionwise feed forward layer.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input dimension.\n",
    "        hidden_dim (int): hidden dimension.\n",
    "        dropout (float, optional): dropout probability. (Default: 0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(input_dim),\n",
    "            torch.nn.Linear(input_dim, hidden_dim, bias=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, input_dim, bias=True),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): with shape `(*, D)`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output, with shape `(*, D)`.\n",
    "        \"\"\"\n",
    "        return self.sequential(input)\n",
    "\n",
    "\n",
    "class ConformerLayer(torch.nn.Module):\n",
    "    r\"\"\"Conformer layer that constitutes Conformer.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input dimension.\n",
    "        ffn_dim (int): hidden layer dimension of feedforward network.\n",
    "        num_attention_heads (int): number of attention heads.\n",
    "        depthwise_conv_kernel_size (int): kernel size of depthwise convolution layer.\n",
    "        dropout (float, optional): dropout probability. (Default: 0.0)\n",
    "        use_group_norm (bool, optional): use ``GroupNorm`` rather than ``BatchNorm1d``\n",
    "            in the convolution module. (Default: ``False``)\n",
    "        convolution_first (bool, optional): apply the convolution module ahead of\n",
    "            the attention module. (Default: ``False``)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        ffn_dim: int,\n",
    "        num_attention_heads: int,\n",
    "        depthwise_conv_kernel_size: int,\n",
    "        dropout: float = 0.0,\n",
    "        use_group_norm: bool = False,\n",
    "        convolution_first: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffn1 = _FeedForwardModule(input_dim, ffn_dim, dropout=dropout)\n",
    "\n",
    "        self.self_attn_layer_norm = torch.nn.LayerNorm(input_dim)\n",
    "        self.self_attn = torch.nn.MultiheadAttention(input_dim, num_attention_heads, dropout=dropout)\n",
    "        self.self_attn_dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.conv_module = _ConvolutionModule(\n",
    "            input_dim=input_dim,\n",
    "            num_channels=input_dim,\n",
    "            depthwise_kernel_size=depthwise_conv_kernel_size,\n",
    "            dropout=dropout,\n",
    "            bias=True,\n",
    "            use_group_norm=use_group_norm,\n",
    "        )\n",
    "\n",
    "        self.ffn2 = _FeedForwardModule(input_dim, ffn_dim, dropout=dropout)\n",
    "        self.final_layer_norm = torch.nn.LayerNorm(input_dim)\n",
    "        self.convolution_first = convolution_first\n",
    "\n",
    "    def _apply_convolution(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        residual = input\n",
    "        input = input.transpose(0, 1)\n",
    "        input = self.conv_module(input)\n",
    "        input = input.transpose(0, 1)\n",
    "        input = residual + input\n",
    "        return input\n",
    "\n",
    "    def forward(self, input: torch.Tensor, key_padding_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): input, with shape `(T, B, D)`.\n",
    "            key_padding_mask (torch.Tensor or None): key padding mask to use in self attention layer.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: output, with shape `(T, B, D)`.\n",
    "        \"\"\"\n",
    "        residual = input\n",
    "        x = self.ffn1(input)\n",
    "        x = x * 0.5 + residual\n",
    "\n",
    "        if self.convolution_first:\n",
    "            x = self._apply_convolution(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "        x, _ = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=False,\n",
    "        )\n",
    "        x = self.self_attn_dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        if not self.convolution_first:\n",
    "            x = self._apply_convolution(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.ffn2(x)\n",
    "        x = x * 0.5 + residual\n",
    "\n",
    "        x = self.final_layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conformer(torch.nn.Module):\n",
    "    r\"\"\"Conformer architecture introduced in\n",
    "    *Conformer: Convolution-augmented Transformer for Speech Recognition*\n",
    "    :cite:`gulati2020conformer`.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): input dimension.\n",
    "        num_heads (int): number of attention heads in each Conformer layer.\n",
    "        ffn_dim (int): hidden layer dimension of feedforward networks.\n",
    "        num_layers (int): number of Conformer layers to instantiate.\n",
    "        depthwise_conv_kernel_size (int): kernel size of each Conformer layer's depthwise convolution layer.\n",
    "        dropout (float, optional): dropout probability. (Default: 0.0)\n",
    "        use_group_norm (bool, optional): use ``GroupNorm`` rather than ``BatchNorm1d``\n",
    "            in the convolution module. (Default: ``False``)\n",
    "        convolution_first (bool, optional): apply the convolution module ahead of\n",
    "            the attention module. (Default: ``False``)\n",
    "\n",
    "    Examples:\n",
    "        >>> conformer = Conformer(\n",
    "        >>>     input_dim=80,\n",
    "        >>>     num_heads=4,\n",
    "        >>>     ffn_dim=128,\n",
    "        >>>     num_layers=4,\n",
    "        >>>     depthwise_conv_kernel_size=31,\n",
    "        >>> )\n",
    "        >>> lengths = torch.randint(1, 400, (10,))  # (batch,)\n",
    "        >>> input = torch.rand(10, int(lengths.max()), input_dim)  # (batch, num_frames, input_dim)\n",
    "        >>> output = conformer(input, lengths)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_heads: int,\n",
    "        ffn_dim: int,\n",
    "        num_layers: int,\n",
    "        depthwise_conv_kernel_size: int,\n",
    "        dropout: float = 0.0,\n",
    "        use_group_norm: bool = False,\n",
    "        convolution_first: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conformer_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                ConformerLayer(\n",
    "                    input_dim,\n",
    "                    ffn_dim,\n",
    "                    num_heads,\n",
    "                    depthwise_conv_kernel_size,\n",
    "                    dropout=dropout,\n",
    "                    use_group_norm=use_group_norm,\n",
    "                    convolution_first=convolution_first,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "    def _lengths_to_padding_mask(self, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = lengths.shape[0]\n",
    "        max_length = int(torch.max(lengths).item())\n",
    "        padding_mask = torch.arange(max_length, device=lengths.device, dtype=lengths.dtype).expand(\n",
    "            batch_size, max_length\n",
    "        ) >= lengths.unsqueeze(1)\n",
    "        return padding_mask\n",
    "    \n",
    "\n",
    "    def forward(self, input: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): with shape `(B, T, input_dim)`.\n",
    "            lengths (torch.Tensor): with shape `(B,)` and i-th element representing\n",
    "                number of valid frames for i-th batch element in ``input``.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor, torch.Tensor)\n",
    "                torch.Tensor\n",
    "                    output frames, with shape `(B, T, input_dim)`\n",
    "                torch.Tensor\n",
    "                    output lengths, with shape `(B,)` and i-th element representing\n",
    "                    number of valid frames for i-th batch element in output frames.\n",
    "        \"\"\"\n",
    "        encoder_padding_mask = self._lengths_to_padding_mask(lengths)\n",
    "\n",
    "        x = input.transpose(0, 1)\n",
    "        for layer in self.conformer_layers:\n",
    "            x = layer(x, encoder_padding_mask)\n",
    "        return x.transpose(0, 1), lengths, encoder_padding_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71dc467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, max_len, dropout ):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size) ## shape = (1,256)\n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1) ## shape (maxlen, 1)\n",
    "        \n",
    "        pos_embedding = torch.zeros((max_len, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding) ## opimizer가 업데이트 하지 않음,\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx = 1) ## 단어사전 \n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "    \n",
    "class pre_TokenEmbedding(nn.Module): ## pretrain_word embedding\n",
    "    def __init__(self, vocab, emb_size):\n",
    "        super(pre_TokenEmbedding,self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors, padding_idx=1, freeze= False)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 emb_size, \n",
    "                 nhead,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 max_len,\n",
    "                 pre_vector,\n",
    "                 dim_feedforward = 512, \n",
    "                 dropout = 0.4):\n",
    "        super(Seq2SeqTransformer,self).__init__()\n",
    "        \n",
    "        self.pre_vector = pre_vector\n",
    "        \n",
    "        self.conformer = Conformer(input_dim = emb_size,\n",
    "                                   num_heads=nhead,\n",
    "                                   ffn_dim = dim_feedforward,\n",
    "                                   num_layers = num_encoder_layers,\n",
    "                                   depthwise_conv_kernel_size = 31, \n",
    "                                   dropout = dropout)\n",
    "        \n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model = emb_size,\n",
    "                                                        dim_feedforward = dim_feedforward,\n",
    "                                                        dropout= dropout,\n",
    "                                                       nhead = nhead,\n",
    "                                                       device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        \n",
    "\n",
    "        self.src_tok_emb = pre_TokenEmbedding(pre_vector, emb_size)\n",
    "        self.tgt_tok_emb = pre_TokenEmbedding(pre_vector, emb_size)\n",
    "\n",
    "            \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, max_len, dropout = dropout)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask, tgt_padding_mask, lengths):\n",
    "\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src)) # seq, batch, dim\n",
    "        src_emb = src_emb.permute(1,0,2) ## batch, seq, dim\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        memory = self.conformer(src_emb, lengths) ## batch, seq, dim\n",
    "        encoder_padding_mask = memory[2]\n",
    "        memory = memory[0].permute(1,0,2) ## seq, batch, dim\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask = tgt_mask, \n",
    "                                        tgt_key_padding_mask = tgt_padding_mask, \n",
    "                                        memory_key_padding_mask = encoder_padding_mask)\n",
    "        return self.generator(outs)\n",
    "        \n",
    "    def encode(self, src, lengths):\n",
    "        return self.conformer(self.positional_encoding(self.src_tok_emb(src)).permute(1,0,2), lengths)\n",
    "    \n",
    "    def decoder(self,tgt, memory, tgt_mask):\n",
    "        return self.transformer_decoder(self.positional_encoding(self.tgt_tok_emb(tgt)),\n",
    "                                       memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b01c97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denti_chatbot(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Denti_chatbot,self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mecab = Mecab()\n",
    "        \n",
    "        self.soy_model_patient1 = CountSpace()\n",
    "        self.soy_model_patient2 = CountSpace()\n",
    "        self.soy_model_patient3 = CountSpace()\n",
    "        self.soy_model_patient4 = CountSpace()\n",
    "        self.soy_model_patient5 = CountSpace()\n",
    "        \n",
    "        self.soy_model_patient1.load_model('/data/kmkim/chatbot_data_model/dental_5/5_data/prepro/patient_1_patient_data_soy.model',\n",
    "                                           json_format = False)\n",
    "        self.soy_model_patient2.load_model('/data/kmkim/chatbot_data_model/dental_5/5_data/prepro/patient_2_patient_data_soy.model',\n",
    "                                           json_format = False)\n",
    "        self.soy_model_patient3.load_model('/data/kmkim/chatbot_data_model/dental_5/5_data/prepro/patient_3_patient_data_soy.model',\n",
    "                                           json_format = False)\n",
    "        self.soy_model_patient4.load_model('/data/kmkim/chatbot_data_model/dental_5/5_data/prepro/patient_4_patient_data_soy.model',\n",
    "                                           json_format = False)\n",
    "        self.soy_model_patient5.load_model('/data/kmkim/chatbot_data_model/dental_5/5_data/prepro/patient_5_patient_data_soy.model',\n",
    "                                           json_format = False)\n",
    "\n",
    "\n",
    "        self.pre_vocab, self.pre_vector = self.load_embedding()\n",
    "        \n",
    "\n",
    "        self.EMB_SIZE = 256\n",
    "        self.NHEAD = 8\n",
    "        self.FFN_HID_DIM = 512\n",
    "        self.BATCH_SIZE = 128\n",
    "        self.NUM_ENCODER_LAYERS = 4 ## 새로운 모델 레이어로 정정 필요\n",
    "        self.NUM_DECODER_LAYERS = 4 ## 위와 같음\n",
    "        self.max_length = 54\n",
    "        \n",
    "        \n",
    "        self.transformer_p1 = Seq2SeqTransformer(self.NUM_ENCODER_LAYERS, self.NUM_DECODER_LAYERS, self.EMB_SIZE, self.NHEAD, len(self.pre_vocab['patient1']),\n",
    "                                                 len(self.pre_vocab['patient1']),self.max_length, self.pre_vector['patient1'], self.FFN_HID_DIM)\n",
    "        self.transformer_p1 = self.transformer_p1.to(self.device)\n",
    "        self.transformer_p1.load_state_dict(torch.load('/data/kmkim/chatbot_data_model/dental_5/conformer_save/Conformer_patient1.pt'))\n",
    "        \n",
    "        self.transformer_p2 = Seq2SeqTransformer(self.NUM_ENCODER_LAYERS, self.NUM_DECODER_LAYERS, self.EMB_SIZE, self.NHEAD, len(self.pre_vocab['patient2']),\n",
    "                                                 len(self.pre_vocab['patient2']),self.max_length, self.pre_vector['patient2'], self.FFN_HID_DIM)\n",
    "        self.transformer_p2 = self.transformer_p2.to(self.device)\n",
    "        self.transformer_p2.load_state_dict(torch.load('/data/kmkim/chatbot_data_model/dental_5/conformer_save/Conformer_patient2.pt'))\n",
    "        \n",
    "        self.transformer_p3 = Seq2SeqTransformer(self.NUM_ENCODER_LAYERS, self.NUM_DECODER_LAYERS, self.EMB_SIZE, self.NHEAD, len(self.pre_vocab['patient3']),\n",
    "                                                 len(self.pre_vocab['patient3']),self.max_length, self.pre_vector['patient3'] ,self.FFN_HID_DIM)\n",
    "        self.transformer_p3 = self.transformer_p3.to(self.device)\n",
    "        self.transformer_p3.load_state_dict(torch.load('/data/kmkim/chatbot_data_model/dental_5/conformer_save/Conformer_patient3.pt'))\n",
    "        \n",
    "        self.transformer_p4 = Seq2SeqTransformer(self.NUM_ENCODER_LAYERS, self.NUM_DECODER_LAYERS, self.EMB_SIZE, self.NHEAD, len(self.pre_vocab['patient4']),\n",
    "                                                 len(self.pre_vocab['patient4']),self.max_length, self.pre_vector['patient4'], self.FFN_HID_DIM)\n",
    "        self.transformer_p4 = self.transformer_p4.to(self.device)\n",
    "        self.transformer_p4.load_state_dict(torch.load('/data/kmkim/chatbot_data_model/dental_5/conformer_save/Conformer_patient4.pt'))        \n",
    "        \n",
    "        self.transformer_p5 = Seq2SeqTransformer(self.NUM_ENCODER_LAYERS, self.NUM_DECODER_LAYERS, self.EMB_SIZE, self.NHEAD, len(self.pre_vocab['patient5']),\n",
    "                                                 len(self.pre_vocab['patient5']),self.max_length, self.pre_vector['patient5'], self.FFN_HID_DIM)\n",
    "        self.transformer_p5 = self.transformer_p5.to(self.device)\n",
    "        self.transformer_p5.load_state_dict(torch.load('/data/kmkim/chatbot_data_model/dental_5/conformer_save/Conformer_patient5.pt'))\n",
    "        \n",
    " \n",
    "\n",
    "        \n",
    "        \n",
    "    def tokenizer(self, sentence):\n",
    "        return self.mecab.morphs(sentence)\n",
    "\n",
    "    def sequential_transforms(self, *transforms):\n",
    "        def func(txt_input):\n",
    "            for transform in transforms:\n",
    "                txt_input = transform(txt_input)\n",
    "            return txt_input\n",
    "        return func\n",
    "\n",
    "    def tensor_transform(self,token_ids):\n",
    "        return torch.cat((torch.tensor([2]),\n",
    "                         torch.tensor(token_ids),\n",
    "                         torch.tensor([3])))\n",
    "\n",
    "    def load_embedding(self):\n",
    "\n",
    "        os.chdir(\"/data/kmkim/chatbot_data_model/dental_5/5_data/prepro\")\n",
    "        \n",
    "        patient1_que_emb= torchtext.vocab.Vectors('FastText_patient_1_patient_data') ## FastText_renew_chat_bot\n",
    "        patient1_que_vocab = torchtext.vocab.vocab(patient1_que_emb.stoi, min_freq= 0, specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "\n",
    "        patient2_que_emb= torchtext.vocab.Vectors('FastText_patient_2_patient_data')\n",
    "        patient2_que_vocab = torchtext.vocab.vocab(patient2_que_emb.stoi, min_freq= 0, specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "\n",
    "        patient3_que_emb= torchtext.vocab.Vectors('FastText_patient_3_patient_data')\n",
    "        patient3_que_vocab = torchtext.vocab.vocab(patient3_que_emb.stoi, min_freq= 0, specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "\n",
    "        patient4_que_emb= torchtext.vocab.Vectors('FastText_patient_4_patient_data')\n",
    "        patient4_que_vocab = torchtext.vocab.vocab(patient4_que_emb.stoi, min_freq= 0, specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "        \n",
    "        patient5_que_emb= torchtext.vocab.Vectors('FastText_patient_5_patient_data')\n",
    "        patient5_que_vocab = torchtext.vocab.vocab(patient5_que_emb.stoi, min_freq= 0, specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "\n",
    "\n",
    "        #equ_vocab = torchtext.vocab.vocab(equ_emb.stoi, min_freq= 0)\n",
    "\n",
    "        vector_dic = {'patient1': patient1_que_emb, 'patient2': patient2_que_emb, 'patient3': patient3_que_emb,\n",
    "                      'patient4': patient4_que_emb, 'patient5': patient5_que_emb} \n",
    "\n",
    "        vocab_dic = {'patient1': patient1_que_vocab, 'patient2': patient2_que_vocab, 'patient3': patient3_que_vocab,\n",
    "                      'patient4': patient4_que_vocab, 'patient5': patient5_que_vocab}\n",
    "\n",
    "        for ln in ['patient1', 'patient2','patient3','patient4','patient5']:\n",
    "            a = torch.zeros(4,256, requires_grad = False)                                 \n",
    "            vector_dic[ln].vectors = torch.cat([a, vector_dic[ln].vectors], dim=0) \n",
    "            vector_dic[ln].stoi = dict(zip( vector_dic[ln].stoi.keys(), map(lambda x:x[1]+4,  vector_dic[ln].stoi.items()))) \n",
    "            for i, j in enumerate([\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]):\n",
    "                 vector_dic[ln].stoi[j] = i                 \n",
    "            vocab_dic[ln].set_default_index(0)                    \n",
    "\n",
    "        return vocab_dic, vector_dic\n",
    "    \n",
    "    def greedy_decode(self, model, lengths, src, max_len, start_symbol):\n",
    "        memory = model.encode(src, lengths)[0].permute(1,0,2)\n",
    "        ys = torch.ones(1,src.size(1)).fill_(start_symbol).type(torch.long).to(self.device) \n",
    "        for i in range(max_len):\n",
    "            tgt_mask = (self.generate_square_subsequent_mask(ys.size(0)))\n",
    "            out = model.decoder(ys, memory, tgt_mask)\n",
    "            out = out.transpose(0,1) # (sequence ,batch, hidden)  -> (batch,sequence ,hidden)\n",
    "            prob = model.generator(out[:, -1]) ## 마지막 단어\n",
    "            _, next_word = torch.max(prob, dim = 1)\n",
    "            if next_word == 3:\n",
    "                break\n",
    "            next_word = next_word.unsqueeze(0)\n",
    "            ys = torch.cat([ys, next_word], dim = 0)\n",
    "        return ys\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones((sz,sz), device = self.device ))== 1).T \n",
    "        mask = mask.float().masked_fill(mask==0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "\n",
    "    def translate(self, model, src_sentence, pre_vocab, condition):\n",
    "    \n",
    "        model.eval()\n",
    "        src = self.text_transform(src_sentence).view(-1, 1)\n",
    "        src = src.to(self.device)\n",
    "        \n",
    "        num_tokens = src.shape[0]\n",
    "        num_tokens = torch.tensor([num_tokens]).to(self.device)\n",
    "        \n",
    "        tgt_tokens = self.greedy_decode(\n",
    "            model, num_tokens,  src, max_len=num_tokens + 4, start_symbol=2).flatten()\n",
    "        \n",
    "        ans = (\"\".join(pre_vocab.lookup_tokens(tgt_tokens.tolist()))).replace(\"<sos>\", \"\").replace(\"<eos>\",\"\").replace(\"<pad>\", \"\")\n",
    "        if condition == 1:\n",
    "            ans = self.soy_model_patient1.correct(ans, min_count = 1,force_abs_threshold=0.3, nonspace_threshold= -0.3, space_threshold= 0.3)[0]\n",
    "        elif condition == 2:\n",
    "            ans = self.soy_model_patient2.correct(ans, min_count = 1,force_abs_threshold=0.3, nonspace_threshold= -0.3, space_threshold= 0.3)[0]\n",
    "        elif condition == 3:\n",
    "            ans = self.soy_model_patient3.correct(ans, min_count = 1,force_abs_threshold=0.3, nonspace_threshold= -0.3, space_threshold= 0.3)[0]\n",
    "        elif condition == 4:\n",
    "            ans = self.soy_model_patient4.correct(ans, min_count = 1,force_abs_threshold=0.3, nonspace_threshold= -0.3, space_threshold= 0.3)[0] \n",
    "        else:\n",
    "            ans = self.soy_model_patient5.correct(ans, min_count = 1,force_abs_threshold=0.3, nonspace_threshold= -0.3, space_threshold= 0.3)[0]\n",
    "        return ans\n",
    "        \n",
    "    def forward(self, text, condition):\n",
    "        if condition == 1:\n",
    "\n",
    "            self.text_transform = self.sequential_transforms(self.tokenizer, self.pre_vocab['patient1'], self.tensor_transform)\n",
    "            ans = self.translate(self.transformer_p1, text, self.pre_vocab['patient1'], condition)\n",
    "        \n",
    "        elif condition == 2:\n",
    "            self.text_transform = self.sequential_transforms(self.tokenizer, self.pre_vocab['patient2'], self.tensor_transform)\n",
    "            ans = self.translate(self.transformer_p2, text, self.pre_vocab['patient2'], condition)\n",
    "         \n",
    "        elif condition == 3:\n",
    "            self.text_transform = self.sequential_transforms(self.tokenizer, self.pre_vocab['patient3'], self.tensor_transform)\n",
    "            ans = self.translate(self.transformer_p3, text, self.pre_vocab['patient3'], condition)\n",
    "        \n",
    "        elif condition == 4:\n",
    "            self.text_transform = self.sequential_transforms(self.tokenizer, self.pre_vocab['patient4'], self.tensor_transform)\n",
    "            ans = self.translate(self.transformer_p4, text, self.pre_vocab['patient4'], condition)\n",
    "            \n",
    "        else:\n",
    "            self.text_transform= self.sequential_transforms(self.tokenizer, self.pre_vocab['patient5'], self.tensor_transform)\n",
    "            ans = self.translate(self.transformer_p5, text, self.pre_vocab['patient5'], condition)\n",
    "\n",
    "\n",
    "        return ans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5490022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
